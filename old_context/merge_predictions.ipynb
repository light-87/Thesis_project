{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095db266-9496-4a91-8fbd-2f3d64a129c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Create a directory for our analysis results\n",
    "os.makedirs(\"ensemble_analysis\", exist_ok=True)\n",
    "os.makedirs(\"ensemble_analysis/figures\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c54681-9179-4594-a73a-56b9028e7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load XGBoost and Transformer predictions\n",
    "def load_predictions():\n",
    "    # Load Transformer predictions\n",
    "    transformer_preds = pd.read_csv(\"transformers_logs/results/test_predictions.csv\")\n",
    "    \n",
    "    # Load XGBoost predictions\n",
    "    # Assuming the XGBoost predictions are stored in a similar format\n",
    "    # If format is different, this will need to be modified\n",
    "    try:\n",
    "        xgb_preds = pd.read_csv(\"results/test_predictions.csv\")\n",
    "    except FileNotFoundError:\n",
    "        # Try alternative formats that might exist\n",
    "        try:\n",
    "            # Look for any JSON files in results directory\n",
    "            json_files = [f for f in os.listdir(\"results\") if f.endswith(\".json\") and \"metrics\" in f]\n",
    "            if json_files:\n",
    "                # Sort by date (assuming naming includes date)\n",
    "                json_files.sort(reverse=True)\n",
    "                # Load the most recent metrics file\n",
    "                with open(f\"results/{json_files[0]}\", \"r\") as f:\n",
    "                    xgb_metrics = json.load(f)\n",
    "                \n",
    "                # For now, use transformer predictions structure but will replace with actual XGB predictions\n",
    "                print(f\"Found XGBoost metrics but no predictions file. Will need to generate predictions.\")\n",
    "                xgb_preds = pd.read_csv(\"split_data/test_data.csv\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No XGBoost prediction or metrics files found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading XGBoost predictions: {e}\")\n",
    "            print(\"Will attempt to load and run XGBoost model to generate predictions.\")\n",
    "            # Create a placeholder with transformer structure for now\n",
    "            xgb_preds = transformer_preds[['header', 'position', 'target']].copy()\n",
    "    \n",
    "    return transformer_preds, xgb_preds\n",
    "\n",
    "# Load model predictions\n",
    "transformer_preds, xgb_preds = load_predictions()\n",
    "\n",
    "# Display the first few rows of each prediction dataframe\n",
    "print(\"Transformer Predictions:\")\n",
    "display(transformer_preds.head())\n",
    "\n",
    "print(\"\\nXGBoost Predictions or Test Data:\")\n",
    "display(xgb_preds.head())\n",
    "\n",
    "# Check for column name inconsistencies\n",
    "print(\"\\nTransformer columns:\", transformer_preds.columns.tolist())\n",
    "print(\"XGBoost columns:\", xgb_preds.columns.tolist())\n",
    "\n",
    "# Check if we need to identify overlap between the two sets\n",
    "transformer_id_cols = [col for col in transformer_preds.columns if col.lower() in ['header', 'uniprot id', 'position', 'pos']]\n",
    "xgb_id_cols = [col for col in xgb_preds.columns if col.lower() in ['header', 'uniprot id', 'position', 'pos']]\n",
    "\n",
    "print(\"\\nTransformer ID columns:\", transformer_id_cols)\n",
    "print(\"XGBoost ID columns:\", xgb_id_cols)\n",
    "\n",
    "# Try to determine common identifier columns \n",
    "header_col_transformer = [col for col in transformer_id_cols if 'header' in col.lower() or 'id' in col.lower()]\n",
    "header_col_xgb = [col for col in xgb_id_cols if 'header' in col.lower() or 'id' in col.lower()]\n",
    "pos_col_transformer = [col for col in transformer_id_cols if 'pos' in col.lower()]\n",
    "pos_col_xgb = [col for col in xgb_id_cols if 'pos' in col.lower()]\n",
    "\n",
    "if header_col_transformer and header_col_xgb and pos_col_transformer and pos_col_xgb:\n",
    "    header_col_transformer = header_col_transformer[0]\n",
    "    header_col_xgb = header_col_xgb[0]\n",
    "    pos_col_transformer = pos_col_transformer[0]\n",
    "    pos_col_xgb = pos_col_xgb[0]\n",
    "    \n",
    "    # Check overlap between datasets\n",
    "    transformer_ids = set(zip(transformer_preds[header_col_transformer], transformer_preds[pos_col_transformer]))\n",
    "    xgb_ids = set(zip(xgb_preds[header_col_xgb], xgb_preds[pos_col_xgb]))\n",
    "    \n",
    "    overlap = transformer_ids.intersection(xgb_ids)\n",
    "    \n",
    "    print(f\"\\nTransformer dataset has {len(transformer_ids)} unique samples\")\n",
    "    print(f\"XGBoost dataset has {len(xgb_ids)} unique samples\")\n",
    "    print(f\"Overlap: {len(overlap)} samples ({len(overlap)/len(transformer_ids)*100:.2f}% of transformer, {len(overlap)/len(xgb_ids)*100:.2f}% of XGBoost)\")\n",
    "else:\n",
    "    print(\"\\nCould not automatically determine ID columns. Please check the dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a2da5-49f3-47fe-9fac-cda5a9391fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare the XGBoost model if needed\n",
    "def load_xgboost_model_and_predict():\n",
    "    print(\"Loading XGBoost model and making predictions...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load the model\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(\"results/phosphorylation_xgb_model.json\")\n",
    "        \n",
    "        # Load test data\n",
    "        test_data = pd.read_csv(\"split_data/test_data.csv\")\n",
    "        \n",
    "        # Prepare data for prediction\n",
    "        target_col = 'target'\n",
    "        id_cols = ['Header', 'Position']\n",
    "        \n",
    "        # Identify feature columns (anything not in id_cols or target_col)\n",
    "        feature_cols = [col for col in test_data.columns \n",
    "                        if col not in id_cols + [target_col]]\n",
    "        \n",
    "        if not feature_cols:\n",
    "            print(\"No feature columns found in test data!\")\n",
    "            return None\n",
    "            \n",
    "        X_test = test_data[feature_cols]\n",
    "        \n",
    "        # Convert to DMatrix\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        xgb_probs = xgb_model.predict(dtest)\n",
    "        \n",
    "        # Create prediction dataframe\n",
    "        xgb_preds = pd.DataFrame({\n",
    "            'Header': test_data['Header'],\n",
    "            'Position': test_data['Position'],\n",
    "            'target': test_data['target'],\n",
    "            'probability': xgb_probs,\n",
    "            'prediction': (xgb_probs > 0.5).astype(int)\n",
    "        })\n",
    "        \n",
    "        print(\"XGBoost predictions generated successfully\")\n",
    "        return xgb_preds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or running XGBoost model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to merge predictions, ensuring only common samples are used\n",
    "def merge_predictions(transformer_preds, xgb_preds):\n",
    "    # Identify key columns for merging\n",
    "    transformer_cols = transformer_preds.columns\n",
    "    xgb_cols = xgb_preds.columns\n",
    "    \n",
    "    # Try to find ID columns automatically\n",
    "    header_col_transformer = [col for col in transformer_cols if 'header' in col.lower() or 'id' in col.lower()]\n",
    "    pos_col_transformer = [col for col in transformer_cols if 'pos' in col.lower()]\n",
    "    header_col_xgb = [col for col in xgb_cols if 'header' in col.lower() or 'id' in col.lower()]\n",
    "    pos_col_xgb = [col for col in xgb_cols if 'pos' in col.lower()]\n",
    "    \n",
    "    # Check if we found the columns\n",
    "    if (not header_col_transformer or not pos_col_transformer or \n",
    "        not header_col_xgb or not pos_col_xgb):\n",
    "        print(\"Could not automatically identify ID columns. Using default column names.\")\n",
    "        # Use default column names\n",
    "        header_col_transformer = 'header'\n",
    "        pos_col_transformer = 'position'\n",
    "        header_col_xgb = 'Header'\n",
    "        pos_col_xgb = 'Position'\n",
    "    else:\n",
    "        header_col_transformer = header_col_transformer[0]\n",
    "        pos_col_transformer = pos_col_transformer[0]\n",
    "        header_col_xgb = header_col_xgb[0]\n",
    "        pos_col_xgb = pos_col_xgb[0]\n",
    "    \n",
    "    print(f\"Merging on {header_col_transformer}/{pos_col_transformer} from transformer and {header_col_xgb}/{pos_col_xgb} from XGBoost\")\n",
    "    \n",
    "    # Check if each dataframe has the necessary columns\n",
    "    for df, name, header_col, pos_col in [\n",
    "        (transformer_preds, \"Transformer\", header_col_transformer, pos_col_transformer),\n",
    "        (xgb_preds, \"XGBoost\", header_col_xgb, pos_col_xgb)\n",
    "    ]:\n",
    "        if header_col not in df.columns:\n",
    "            print(f\"Error: {header_col} not found in {name} predictions\")\n",
    "            return None\n",
    "        if pos_col not in df.columns:\n",
    "            print(f\"Error: {pos_col} not found in {name} predictions\")\n",
    "            return None\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_preds = pd.merge(\n",
    "        transformer_preds,\n",
    "        xgb_preds,\n",
    "        left_on=[header_col_transformer, pos_col_transformer],\n",
    "        right_on=[header_col_xgb, pos_col_xgb],\n",
    "        suffixes=('_transformer', '_xgb')\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully merged {len(merged_preds)} samples\")\n",
    "    \n",
    "    # Check for target column inconsistencies\n",
    "    target_col_transformer = [col for col in transformer_cols if 'target' in col.lower()]\n",
    "    target_col_xgb = [col for col in xgb_cols if 'target' in col.lower()]\n",
    "    \n",
    "    if target_col_transformer and target_col_xgb:\n",
    "        target_col_transformer = target_col_transformer[0]\n",
    "        target_col_xgb = target_col_xgb[0]\n",
    "        \n",
    "        # Check if targets match\n",
    "        if target_col_transformer in merged_preds and target_col_xgb in merged_preds:\n",
    "            targets_match = (merged_preds[target_col_transformer] == merged_preds[target_col_xgb]).all()\n",
    "            if not targets_match:\n",
    "                print(\"Warning: Target values differ between the two prediction sets!\")\n",
    "            else:\n",
    "                print(\"Target values are consistent between the datasets\")\n",
    "    \n",
    "    return merged_preds\n",
    "\n",
    "# Check if XGBoost predictions need to be generated\n",
    "if ('probability' not in xgb_preds.columns or 'prediction' not in xgb_preds.columns):\n",
    "    print(\"XGBoost predictions need to be generated\")\n",
    "    generated_preds = load_xgboost_model_and_predict()\n",
    "    if generated_preds is not None:\n",
    "        xgb_preds = generated_preds\n",
    "\n",
    "# Merge predictions\n",
    "merged_preds = merge_predictions(transformer_preds, xgb_preds)\n",
    "\n",
    "if merged_preds is not None:\n",
    "    # Display the merged dataframe\n",
    "    print(\"\\nMerged predictions:\")\n",
    "    display(merged_preds.head())\n",
    "    print(f\"Shape: {merged_preds.shape}\")\n",
    "    \n",
    "    # Save merged predictions for later use\n",
    "    merged_preds.to_csv(\"ensemble_analysis/merged_predictions.csv\", index=False)\n",
    "else:\n",
    "    print(\"Failed to merge prediction datasets. Please check the data and columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5a7d8-8ef5-4a2b-ab4a-00f25fefc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we successfully merged the predictions, proceed with ensemble methods\n",
    "if merged_preds is not None:\n",
    "    # Identify target and prediction columns\n",
    "    # Try to find them automatically\n",
    "    target_cols = [col for col in merged_preds.columns if 'target' in col.lower()]\n",
    "    transformer_prob_cols = [col for col in merged_preds.columns \n",
    "                            if 'prob' in col.lower() and 'transformer' in col.lower()]\n",
    "    xgb_prob_cols = [col for col in merged_preds.columns \n",
    "                    if 'prob' in col.lower() and 'xgb' in col.lower()]\n",
    "    \n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "    else:\n",
    "        target_col = 'target'  # default\n",
    "        \n",
    "    if transformer_prob_cols:\n",
    "        transformer_prob_col = transformer_prob_cols[0]\n",
    "    else:\n",
    "        transformer_prob_col = 'probability_transformer'  # default\n",
    "        \n",
    "    if xgb_prob_cols:\n",
    "        xgb_prob_col = xgb_prob_cols[0]\n",
    "    else:\n",
    "        xgb_prob_col = 'probability_xgb'  # default\n",
    "    \n",
    "    print(f\"Using {target_col} as target column\")\n",
    "    print(f\"Using {transformer_prob_col} as transformer probability column\")\n",
    "    print(f\"Using {xgb_prob_col} as XGBoost probability column\")\n",
    "    \n",
    "    # Create prediction columns if they don't exist\n",
    "    if 'prediction_transformer' not in merged_preds.columns:\n",
    "        merged_preds['prediction_transformer'] = (merged_preds[transformer_prob_col] > 0.5).astype(int)\n",
    "    \n",
    "    if 'prediction_xgb' not in merged_preds.columns:\n",
    "        merged_preds['prediction_xgb'] = (merged_preds[xgb_prob_col] > 0.5).astype(int)\n",
    "    \n",
    "    # Function to evaluate model performance\n",
    "    def evaluate_model(y_true, y_pred, y_prob, model_name):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        \n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'AUC': roc_auc\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # Evaluate both models\n",
    "    metrics_transformer = evaluate_model(\n",
    "        merged_preds[target_col], \n",
    "        merged_preds['prediction_transformer'], \n",
    "        merged_preds[transformer_prob_col],\n",
    "        'Transformer'\n",
    "    )\n",
    "    \n",
    "    metrics_xgb = evaluate_model(\n",
    "        merged_preds[target_col], \n",
    "        merged_preds['prediction_xgb'], \n",
    "        merged_preds[xgb_prob_col],\n",
    "        'XGBoost'\n",
    "    )\n",
    "    \n",
    "    # Combine metrics into a dataframe\n",
    "    metrics_df = pd.DataFrame([metrics_transformer, metrics_xgb])\n",
    "    print(\"Performance metrics:\")\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df.to_csv(\"ensemble_analysis/base_model_metrics.csv\", index=False)\n",
    "    \n",
    "    # Now we can proceed with analysis and ensemble methods\n",
    "    print(\"Ready to proceed with analysis and ensemble methods\")\n",
    "else:\n",
    "    print(\"Cannot proceed with analysis as predictions could not be merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0521b-7e46-4e87-8dc9-185cc57a8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\", \n",
    "        cbar=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(title)\n",
    "    return cm\n",
    "\n",
    "# Create confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cm_transformer = plot_confusion_matrix(\n",
    "    merged_preds[target_col], \n",
    "    merged_preds['prediction_transformer'], \n",
    "    'Transformer Confusion Matrix', \n",
    "    axes[0]\n",
    ")\n",
    "\n",
    "cm_xgb = plot_confusion_matrix(\n",
    "    merged_preds[target_col], \n",
    "    merged_preds['prediction_xgb'], \n",
    "    'XGBoost Confusion Matrix', \n",
    "    axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a matrix showing model agreement and disagreement\n",
    "agreement_df = pd.DataFrame({\n",
    "    'XGBoost': merged_preds['prediction_xgb'],\n",
    "    'Transformer': merged_preds['prediction_transformer'],\n",
    "    'True': merged_preds[target_col]\n",
    "})\n",
    "\n",
    "# Count occurrences of each combination\n",
    "agreement_counts = agreement_df.groupby(['XGBoost', 'Transformer', 'True']).size().reset_index(name='Count')\n",
    "print(\"\\nModel agreement and disagreement:\")\n",
    "display(agreement_counts)\n",
    "\n",
    "# Plot model agreement\n",
    "plt.figure(figsize=(10, 8))\n",
    "agreement_pivot = pd.pivot_table(\n",
    "    agreement_counts,\n",
    "    index=['XGBoost', 'Transformer'],\n",
    "    columns='True',\n",
    "    values='Count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Plot as heatmap\n",
    "sns.heatmap(agreement_pivot, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
    "plt.title('Model Agreement vs. True Labels')\n",
    "plt.xlabel('True Label')\n",
    "plt.ylabel('Predictions (XGBoost, Transformer)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/model_agreement.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9582166-c3eb-49d5-a164-891aa13bf77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where both models fail\n",
    "both_fail = merged_preds[\n",
    "    ((merged_preds['prediction_transformer'] != merged_preds[target_col]) & \n",
    "     (merged_preds['prediction_xgb'] != merged_preds[target_col]))\n",
    "]\n",
    "\n",
    "# Find where both models succeed\n",
    "both_succeed = merged_preds[\n",
    "    ((merged_preds['prediction_transformer'] == merged_preds[target_col]) & \n",
    "     (merged_preds['prediction_xgb'] == merged_preds[target_col]))\n",
    "]\n",
    "\n",
    "# Find where only transformer succeeds\n",
    "only_transformer_succeeds = merged_preds[\n",
    "    ((merged_preds['prediction_transformer'] == merged_preds[target_col]) & \n",
    "     (merged_preds['prediction_xgb'] != merged_preds[target_col]))\n",
    "]\n",
    "\n",
    "# Find where only XGBoost succeeds\n",
    "only_xgb_succeeds = merged_preds[\n",
    "    ((merged_preds['prediction_transformer'] != merged_preds[target_col]) & \n",
    "     (merged_preds['prediction_xgb'] == merged_preds[target_col]))\n",
    "]\n",
    "\n",
    "print(f\"\\nBoth models fail: {len(both_fail)} samples ({len(both_fail)/len(merged_preds)*100:.2f}%)\")\n",
    "print(f\"Both models succeed: {len(both_succeed)} samples ({len(both_succeed)/len(merged_preds)*100:.2f}%)\")\n",
    "print(f\"Only Transformer succeeds: {len(only_transformer_succeeds)} samples ({len(only_transformer_succeeds)/len(merged_preds)*100:.2f}%)\")\n",
    "print(f\"Only XGBoost succeeds: {len(only_xgb_succeeds)} samples ({len(only_xgb_succeeds)/len(merged_preds)*100:.2f}%)\")\n",
    "\n",
    "# Save these results\n",
    "result_sets = {\n",
    "    'both_fail': both_fail,\n",
    "    'both_succeed': both_succeed,\n",
    "    'only_transformer_succeeds': only_transformer_succeeds,\n",
    "    'only_xgb_succeeds': only_xgb_succeeds\n",
    "}\n",
    "\n",
    "for name, df in result_sets.items():\n",
    "    df.to_csv(f\"ensemble_analysis/{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c4fd8-1df6-4dfd-a9ec-5f1a4cb03371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing sequence patterns in failures\n",
    "def analyze_sequence_patterns(dataframe, title):\n",
    "    # Check if 'sequence' column exists\n",
    "    sequence_cols = [col for col in dataframe.columns if 'sequence' in col.lower()]\n",
    "    if not sequence_cols:\n",
    "        print(f\"No sequence data available in {title}\")\n",
    "        return None\n",
    "        \n",
    "    sequence_col = sequence_cols[0]\n",
    "    \n",
    "    # Extract center amino acid from each sequence\n",
    "    try:\n",
    "        # Try to extract center position (assuming sequence is of odd length)\n",
    "        dataframe['center_aa'] = dataframe[sequence_col].apply(\n",
    "            lambda seq: seq[len(seq)//2] if len(seq) % 2 == 1 else seq[len(seq)//2 - 1]\n",
    "        )\n",
    "        \n",
    "        # Count center amino acids\n",
    "        center_aa_counts = dataframe['center_aa'].value_counts().reset_index()\n",
    "        center_aa_counts.columns = ['Amino Acid', 'Count']\n",
    "        center_aa_counts['Percentage'] = center_aa_counts['Count'] / len(dataframe) * 100\n",
    "        \n",
    "        print(f\"\\nCenter amino acid distribution in {title}:\")\n",
    "        display(center_aa_counts)\n",
    "        \n",
    "        # Plot center amino acid distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(x='Amino Acid', y='Count', data=center_aa_counts)\n",
    "        ax.set_title(f'Center Amino Acid Distribution - {title}')\n",
    "        ax.set_xlabel('Amino Acid')\n",
    "        ax.set_ylabel('Count')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, p in enumerate(ax.patches):\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2.,\n",
    "                    height + 0.5,\n",
    "                    f'{center_aa_counts.iloc[i][\"Percentage\"]:.1f}%',\n",
    "                    ha=\"center\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'ensemble_analysis/figures/{title.lower().replace(\" \", \"_\")}_center_aa.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return center_aa_counts\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sequences in {title}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze amino acid patterns where both models fail or succeed\n",
    "if 'sequence' in merged_preds.columns or any('sequence' in col.lower() for col in merged_preds.columns):\n",
    "    both_fail_aa = analyze_sequence_patterns(both_fail, \"Both Models Fail\")\n",
    "    both_succeed_aa = analyze_sequence_patterns(both_succeed, \"Both Models Succeed\")\n",
    "    only_transformer_aa = analyze_sequence_patterns(only_transformer_succeeds, \"Only Transformer Succeeds\")\n",
    "    only_xgb_aa = analyze_sequence_patterns(only_xgb_succeeds, \"Only XGBoost Succeeds\")\n",
    "else:\n",
    "    print(\"No sequence data available for pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b897dc-9908-4086-8f2e-facffe89aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between model predictions\n",
    "corr_predictions = merged_preds[['prediction_transformer', 'prediction_xgb', target_col]].corr()\n",
    "corr_probabilities = merged_preds[[transformer_prob_col, xgb_prob_col, target_col]].corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot prediction correlation\n",
    "sns.heatmap(corr_predictions, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[0])\n",
    "axes[0].set_title('Correlation: Predicted Classes')\n",
    "\n",
    "# Plot probability correlation\n",
    "sns.heatmap(corr_probabilities, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[1])\n",
    "axes[1].set_title('Correlation: Prediction Probabilities')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/prediction_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a092615-7f20-4127-aff6-85bd9536a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple Stacking Ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for stacking\n",
    "X_stack = merged_preds[[transformer_prob_col, xgb_prob_col]]\n",
    "y_stack = merged_preds[target_col]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_stack, y_stack, test_size=0.3, random_state=42, stratify=y_stack\n",
    ")\n",
    "\n",
    "# Train a logistic regression meta-classifier\n",
    "lr_meta = LogisticRegression(random_state=42)\n",
    "lr_meta.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = lr_meta.predict(X_val)\n",
    "y_val_prob = lr_meta.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_lr_meta = evaluate_model(y_val, y_val_pred, y_val_prob, 'Stacking Logistic Regression')\n",
    "\n",
    "# Train a small MLP meta-classifier\n",
    "mlp_meta = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,),\n",
    "    max_iter=1000,\n",
    "    alpha=0.01,\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "mlp_meta.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred_mlp = mlp_meta.predict(X_val)\n",
    "y_val_prob_mlp = mlp_meta.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_mlp_meta = evaluate_model(y_val, y_val_pred_mlp, y_val_prob_mlp, 'Stacking MLP')\n",
    "\n",
    "# Combine metrics into a dataframe\n",
    "stacking_metrics_df = pd.DataFrame([metrics_lr_meta, metrics_mlp_meta])\n",
    "print(\"\\nStacking ensemble performance on validation set:\")\n",
    "display(stacking_metrics_df)\n",
    "\n",
    "# Apply to full dataset for comparison\n",
    "y_pred_lr = lr_meta.predict(X_stack)\n",
    "y_prob_lr = lr_meta.predict_proba(X_stack)[:, 1]\n",
    "metrics_lr_full = evaluate_model(y_stack, y_pred_lr, y_prob_lr, 'Stacking LR (Full)')\n",
    "\n",
    "y_pred_mlp = mlp_meta.predict(X_stack)\n",
    "y_prob_mlp = mlp_meta.predict_proba(X_stack)[:, 1]\n",
    "metrics_mlp_full = evaluate_model(y_stack, y_pred_mlp, y_prob_mlp, 'Stacking MLP (Full)')\n",
    "\n",
    "# Update metrics dataframe with results on the full dataset\n",
    "all_metrics_df = pd.DataFrame([\n",
    "    metrics_transformer, \n",
    "    metrics_xgb, \n",
    "    metrics_lr_full, \n",
    "    metrics_mlp_full\n",
    "])\n",
    "print(\"\\nAll models performance on full dataset:\")\n",
    "display(all_metrics_df)\n",
    "\n",
    "# Save all metrics\n",
    "all_metrics_df.to_csv(\"ensemble_analysis/stacking_ensemble_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bb0e3-8aa0-42f5-9067-96b0d778b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Weighted Ensemble\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Function to create weighted predictions\n",
    "def weighted_ensemble(probs1, probs2, weight):\n",
    "    \"\"\"\n",
    "    Combine probabilities using a weighted average.\n",
    "    weight is the weight for probs1 (1-weight for probs2)\n",
    "    \"\"\"\n",
    "    return weight * probs1 + (1 - weight) * probs2\n",
    "\n",
    "# Function to optimize for accuracy\n",
    "def neg_accuracy_score(weight, probs1, probs2, true_labels):\n",
    "    \"\"\"\n",
    "    Returns negative accuracy (for minimization) of weighted ensemble.\n",
    "    \"\"\"\n",
    "    weighted_probs = weighted_ensemble(probs1, probs2, weight)\n",
    "    pred = (weighted_probs > 0.5).astype(int)\n",
    "    return -accuracy_score(true_labels, pred)\n",
    "\n",
    "# Function to optimize for F1 score\n",
    "def neg_f1_score(weight, probs1, probs2, true_labels):\n",
    "    \"\"\"\n",
    "    Returns negative F1 score (for minimization) of weighted ensemble.\n",
    "    \"\"\"\n",
    "    weighted_probs = weighted_ensemble(probs1, probs2, weight)\n",
    "    pred = (weighted_probs > 0.5).astype(int)\n",
    "    return -f1_score(true_labels, pred)\n",
    "\n",
    "# Find optimal weight using cross-validation\n",
    "def find_optimal_weight(X1, X2, y, metric='accuracy', cv=5):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    best_weights = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X1):\n",
    "        # Split data\n",
    "        X1_train, X1_val = X1[train_idx], X1[val_idx]\n",
    "        X2_train, X2_val = X2[train_idx], X2[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Find best weight for this fold\n",
    "        if metric == 'accuracy':\n",
    "            result = minimize(\n",
    "                neg_accuracy_score, \n",
    "                0.5,  # initial guess\n",
    "                args=(X1_val, X2_val, y_val), \n",
    "                bounds=[(0, 1)]\n",
    "            )\n",
    "        elif metric == 'f1':\n",
    "            result = minimize(\n",
    "                neg_f1_score, \n",
    "                0.5,  # initial guess\n",
    "                args=(X1_val, X2_val, y_val), \n",
    "                bounds=[(0, 1)]\n",
    "            )\n",
    "        \n",
    "        best_weights.append(result['x'][0])\n",
    "    \n",
    "    # Return average of best weights from all folds\n",
    "    return np.mean(best_weights)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "transformer_probs = merged_preds[transformer_prob_col].values\n",
    "xgb_probs = merged_preds[xgb_prob_col].values\n",
    "true_labels = merged_preds[target_col].values\n",
    "\n",
    "# Find optimal weights\n",
    "weight_acc = find_optimal_weight(transformer_probs, xgb_probs, true_labels, 'accuracy')\n",
    "weight_f1 = find_optimal_weight(transformer_probs, xgb_probs, true_labels, 'f1')\n",
    "\n",
    "print(f\"\\nOptimal weight (accuracy) for Transformer: {weight_acc:.4f}, XGBoost: {1-weight_acc:.4f}\")\n",
    "print(f\"Optimal weight (F1 score) for Transformer: {weight_f1:.4f}, XGBoost: {1-weight_f1:.4f}\")\n",
    "\n",
    "# Create weighted ensemble predictions\n",
    "weighted_probs_acc = weighted_ensemble(transformer_probs, xgb_probs, weight_acc)\n",
    "weighted_preds_acc = (weighted_probs_acc > 0.5).astype(int)\n",
    "\n",
    "weighted_probs_f1 = weighted_ensemble(transformer_probs, xgb_probs, weight_f1)\n",
    "weighted_preds_f1 = (weighted_probs_f1 > 0.5).astype(int)\n",
    "\n",
    "# Evaluate weighted ensembles\n",
    "metrics_weighted_acc = evaluate_model(\n",
    "    true_labels, weighted_preds_acc, weighted_probs_acc, 'Weighted Ensemble (Accuracy)'\n",
    ")\n",
    "metrics_weighted_f1 = evaluate_model(\n",
    "    true_labels, weighted_preds_f1, weighted_probs_f1, 'Weighted Ensemble (F1)'\n",
    ")\n",
    "\n",
    "# Add results to metrics dataframe\n",
    "all_metrics_df = pd.concat([\n",
    "    all_metrics_df,\n",
    "    pd.DataFrame([metrics_weighted_acc, metrics_weighted_f1])\n",
    "])\n",
    "\n",
    "print(\"\\nAll models performance including weighted ensembles:\")\n",
    "display(all_metrics_df)\n",
    "\n",
    "# Save updated metrics\n",
    "all_metrics_df.to_csv(\"ensemble_analysis/all_ensemble_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de9a2-aa9b-425d-86a5-083d61dc584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Confidence-based Weighted Ensemble\n",
    "def confidence_weighted_ensemble(probs1, probs2):\n",
    "    \"\"\"\n",
    "    Weight each prediction by its confidence - how far it is from 0.5\n",
    "    \"\"\"\n",
    "    # Calculate confidence for each prediction\n",
    "    conf1 = abs(probs1 - 0.5) * 2  # Scale to 0-1\n",
    "    conf2 = abs(probs2 - 0.5) * 2  # Scale to 0-1\n",
    "    \n",
    "    # Normalize confidences\n",
    "    sum_conf = conf1 + conf2\n",
    "    w1 = conf1 / sum_conf\n",
    "    w2 = conf2 / sum_conf\n",
    "    \n",
    "    # Handle edge case where both confidences are 0\n",
    "    w1 = np.nan_to_num(w1, nan=0.5)\n",
    "    w2 = np.nan_to_num(w2, nan=0.5)\n",
    "    \n",
    "    # Weighted average of probabilities\n",
    "    return w1 * probs1 + w2 * probs2\n",
    "\n",
    "# Create confidence-weighted ensemble predictions\n",
    "conf_weighted_probs = confidence_weighted_ensemble(transformer_probs, xgb_probs)\n",
    "conf_weighted_preds = (conf_weighted_probs > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "metrics_conf_weighted = evaluate_model(\n",
    "    true_labels, conf_weighted_preds, conf_weighted_probs, 'Confidence-Weighted Ensemble'\n",
    ")\n",
    "\n",
    "# Add to metrics dataframe\n",
    "all_metrics_df = pd.concat([\n",
    "    all_metrics_df,\n",
    "    pd.DataFrame([metrics_conf_weighted])\n",
    "])\n",
    "\n",
    "print(\"\\nAll models performance including confidence-weighted ensemble:\")\n",
    "display(all_metrics_df)\n",
    "\n",
    "# Save final metrics\n",
    "all_metrics_df.to_csv(\"ensemble_analysis/final_ensemble_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208d71d-1fa3-412a-9b07-5ff2c2105f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Base models\n",
    "fpr_transformer, tpr_transformer, _ = roc_curve(true_labels, transformer_probs)\n",
    "roc_auc_transformer = auc(fpr_transformer, tpr_transformer)\n",
    "plt.plot(fpr_transformer, tpr_transformer, \n",
    "         label=f'Transformer (AUC = {roc_auc_transformer:.4f})')\n",
    "\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(true_labels, xgb_probs)\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "plt.plot(fpr_xgb, tpr_xgb, \n",
    "         label=f'XGBoost (AUC = {roc_auc_xgb:.4f})')\n",
    "\n",
    "# Ensembles\n",
    "fpr_w_acc, tpr_w_acc, _ = roc_curve(true_labels, weighted_probs_acc)\n",
    "roc_auc_w_acc = auc(fpr_w_acc, tpr_w_acc)\n",
    "plt.plot(fpr_w_acc, tpr_w_acc, \n",
    "         label=f'Weighted (Acc) (AUC = {roc_auc_w_acc:.4f})')\n",
    "\n",
    "fpr_w_f1, tpr_w_f1, _ = roc_curve(true_labels, weighted_probs_f1)\n",
    "roc_auc_w_f1 = auc(fpr_w_f1, tpr_w_f1)\n",
    "plt.plot(fpr_w_f1, tpr_w_f1, \n",
    "         label=f'Weighted (F1) (AUC = {roc_auc_w_f1:.4f})')\n",
    "\n",
    "fpr_conf, tpr_conf, _ = roc_curve(true_labels, conf_weighted_probs)\n",
    "roc_auc_conf = auc(fpr_conf, tpr_conf)\n",
    "plt.plot(fpr_conf, tpr_conf, \n",
    "         label=f'Conf-Weighted (AUC = {roc_auc_conf:.4f})')\n",
    "\n",
    "# Stacking ensembles\n",
    "fpr_lr, tpr_lr, _ = roc_curve(true_labels, y_prob_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, \n",
    "         label=f'Stacking LR (AUC = {roc_auc_lr:.4f})')\n",
    "\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(true_labels, y_prob_mlp)\n",
    "roc_auc_mlp = auc(fpr_mlp, tpr_mlp)\n",
    "plt.plot(fpr_mlp, tpr_mlp, \n",
    "         label=f'Stacking MLP (AUC = {roc_auc_mlp:.4f})')\n",
    "\n",
    "# Add diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/all_models_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create Precision-Recall curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Base models\n",
    "precision_transformer, recall_transformer, _ = precision_recall_curve(true_labels, transformer_probs)\n",
    "pr_auc_transformer = auc(recall_transformer, precision_transformer)\n",
    "plt.plot(recall_transformer, precision_transformer, \n",
    "         label=f'Transformer (AUC = {pr_auc_transformer:.4f})')\n",
    "\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(true_labels, xgb_probs)\n",
    "pr_auc_xgb = auc(recall_xgb, precision_xgb)\n",
    "plt.plot(recall_xgb, precision_xgb, \n",
    "         label=f'XGBoost (AUC = {pr_auc_xgb:.4f})')\n",
    "\n",
    "# Ensembles\n",
    "precision_w_acc, recall_w_acc, _ = precision_recall_curve(true_labels, weighted_probs_acc)\n",
    "pr_auc_w_acc = auc(recall_w_acc, precision_w_acc)\n",
    "plt.plot(recall_w_acc, precision_w_acc, \n",
    "         label=f'Weighted (Acc) (AUC = {pr_auc_w_acc:.4f})')\n",
    "\n",
    "precision_w_f1, recall_w_f1, _ = precision_recall_curve(true_labels, weighted_probs_f1)\n",
    "pr_auc_w_f1 = auc(recall_w_f1, precision_w_f1)\n",
    "plt.plot(recall_w_f1, precision_w_f1, \n",
    "         label=f'Weighted (F1) (AUC = {pr_auc_w_f1:.4f})')\n",
    "\n",
    "precision_conf, recall_conf, _ = precision_recall_curve(true_labels, conf_weighted_probs)\n",
    "pr_auc_conf = auc(recall_conf, precision_conf)\n",
    "plt.plot(recall_conf, precision_conf, \n",
    "         label=f'Conf-Weighted (AUC = {pr_auc_conf:.4f})')\n",
    "\n",
    "# Stacking ensembles\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(true_labels, y_prob_lr)\n",
    "pr_auc_lr = auc(recall_lr, precision_lr)\n",
    "plt.plot(recall_lr, precision_lr, \n",
    "         label=f'Stacking LR (AUC = {pr_auc_lr:.4f})')\n",
    "\n",
    "precision_mlp, recall_mlp, _ = precision_recall_curve(true_labels, y_prob_mlp)\n",
    "pr_auc_mlp = auc(recall_mlp, precision_mlp)\n",
    "plt.plot(recall_mlp, precision_mlp, \n",
    "         label=f'Stacking MLP (AUC = {pr_auc_mlp:.4f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves for All Models')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/all_models_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89179f-58da-4793-9af4-bd6591c8d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for ensemble methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Weighted (Accuracy) Ensemble\n",
    "cm_w_acc = plot_confusion_matrix(\n",
    "    true_labels, weighted_preds_acc, 'Weighted Ensemble (Accuracy)', axes[0, 0]\n",
    ")\n",
    "\n",
    "# Weighted (F1) Ensemble\n",
    "cm_w_f1 = plot_confusion_matrix(\n",
    "    true_labels, weighted_preds_f1, 'Weighted Ensemble (F1)', axes[0, 1]\n",
    ")\n",
    "\n",
    "# Confidence-Weighted Ensemble\n",
    "cm_conf = plot_confusion_matrix(\n",
    "    true_labels, conf_weighted_preds, 'Confidence-Weighted Ensemble', axes[1, 0]\n",
    ")\n",
    "\n",
    "# Stacking LR Ensemble\n",
    "cm_lr = plot_confusion_matrix(\n",
    "    true_labels, y_pred_lr, 'Stacking LR Ensemble', axes[1, 1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/ensemble_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e21a8-ad4c-43b9-a42c-27ce767ad207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Feature-level Fusion - if time permits and if we have access to features\n",
    "try:\n",
    "    # Try to load extracted features\n",
    "    xgb_features = pd.read_csv(\"split_data/test_data.csv\")\n",
    "    \n",
    "    # Check if we have any features\n",
    "    id_cols = ['Header', 'Position', 'target']\n",
    "    feature_cols = [col for col in xgb_features.columns if col not in id_cols]\n",
    "    \n",
    "    if len(feature_cols) > 0:\n",
    "        print(f\"\\nFound {len(feature_cols)} XGBoost features\")\n",
    "        \n",
    "        # Merge features with transformer probabilities\n",
    "        fusion_df = pd.merge(\n",
    "            xgb_features,\n",
    "            merged_preds[['header', 'position', transformer_prob_col]],\n",
    "            left_on=['Header', 'Position'],\n",
    "            right_on=['header', 'position'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        print(f\"Feature fusion dataset size: {fusion_df.shape}\")\n",
    "        \n",
    "        # Prepare data for feature fusion\n",
    "        feature_fusion_cols = feature_cols + [transformer_prob_col]\n",
    "        X_fusion = fusion_df[feature_fusion_cols]\n",
    "        y_fusion = fusion_df['target']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_fusion, y_fusion, test_size=0.3, random_state=42, stratify=y_fusion\n",
    "        )\n",
    "        \n",
    "        # Train a model on the combined features\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf_fusion = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=10, \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_fusion.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_test_pred_rf = rf_fusion.predict(X_test)\n",
    "        y_test_prob_rf = rf_fusion.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics_rf_fusion = evaluate_model(\n",
    "            y_test, y_test_pred_rf, y_test_prob_rf, 'Feature Fusion (RF)'\n",
    "        )\n",
    "        \n",
    "        # Apply to full dataset\n",
    "        y_pred_rf = rf_fusion.predict(X_fusion)\n",
    "        y_prob_rf = rf_fusion.predict_proba(X_fusion)[:, 1]\n",
    "        \n",
    "        # Evaluate on full dataset\n",
    "        metrics_rf_full = evaluate_model(\n",
    "            y_fusion, y_pred_rf, y_prob_rf, 'Feature Fusion (RF) Full'\n",
    "        )\n",
    "        \n",
    "        # Add to metrics dataframe\n",
    "        all_metrics_df = pd.concat([\n",
    "            all_metrics_df,\n",
    "            pd.DataFrame([metrics_rf_full])\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nAll models including feature fusion:\")\n",
    "        display(all_metrics_df)\n",
    "        \n",
    "        # Save updated metrics\n",
    "        all_metrics_df.to_csv(\"ensemble_analysis/all_metrics_with_fusion.csv\", index=False)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        if hasattr(rf_fusion, 'feature_importances_'):\n",
    "            importance = rf_fusion.feature_importances_\n",
    "            indices = np.argsort(importance)[::-1]\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Show top 30 features\n",
    "            top_n = min(30, len(feature_fusion_cols))\n",
    "            plt.bar(range(top_n), importance[indices[:top_n]])\n",
    "            plt.xticks(\n",
    "                range(top_n), \n",
    "                [feature_fusion_cols[i] for i in indices[:top_n]], \n",
    "                rotation=90\n",
    "            )\n",
    "            plt.xlabel('Feature')\n",
    "            plt.ylabel('Importance')\n",
    "            plt.title('Feature Importance in Fusion Model')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('ensemble_analysis/figures/fusion_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Save top features\n",
    "            top_features = pd.DataFrame({\n",
    "                'Feature': [feature_fusion_cols[i] for i in indices],\n",
    "                'Importance': importance[indices]\n",
    "            })\n",
    "            top_features.to_csv('ensemble_analysis/fusion_feature_importance.csv', index=False)\n",
    "            \n",
    "            print(\"\\nTop 10 features in fusion model:\")\n",
    "            display(top_features.head(10))\n",
    "            \n",
    "    else:\n",
    "        print(\"No features found in XGBoost test data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not implement feature-level fusion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef55aad-4e30-48ba-b071-69fdf9cee395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual summary of the performance of all models\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Sort models by F1 score\n",
    "sorted_metrics = all_metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Plot metrics for each model\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Plot metrics as grouped bars\n",
    "bar_width = 0.15\n",
    "index = np.arange(len(sorted_metrics))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(\n",
    "        index + i*bar_width, \n",
    "        sorted_metrics[metric], \n",
    "        bar_width,\n",
    "        label=metric,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Comparison of All Models')\n",
    "plt.xticks(index + bar_width*2, sorted_metrics['Model'], rotation=45, ha='right')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_analysis/figures/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table of relative improvement\n",
    "base_model = 'XGBoost' if metrics_xgb['F1 Score'] > metrics_transformer['F1 Score'] else 'Transformer'\n",
    "base_f1 = metrics_xgb['F1 Score'] if base_model == 'XGBoost' else metrics_transformer['F1 Score']\n",
    "\n",
    "# Calculate improvements\n",
    "improvement_df = all_metrics_df.copy()\n",
    "improvement_df['F1 Improvement'] = improvement_df['F1 Score'] - base_f1\n",
    "improvement_df['F1 Improvement %'] = (improvement_df['F1 Score'] - base_f1) / base_f1 * 100\n",
    "improvement_df = improvement_df.sort_values('F1 Improvement', ascending=False)\n",
    "\n",
    "print(f\"\\nImprovement over best base model ({base_model}, F1={base_f1:.4f}):\")\n",
    "display(improvement_df[['Model', 'F1 Score', 'F1 Improvement', 'F1 Improvement %']])\n",
    "\n",
    "# Save improvement summary\n",
    "improvement_df.to_csv('ensemble_analysis/improvement_summary.csv', index=False)\n",
    "\n",
    "# Create a final textual summary\n",
    "best_model = improvement_df.iloc[0]['Model']\n",
    "best_f1 = improvement_df.iloc[0]['F1 Score']\n",
    "best_improvement = improvement_df.iloc[0]['F1 Improvement %']\n",
    "\n",
    "summary = f\"\"\"\n",
    "# Phosphorylation Site Prediction Ensemble Analysis Summary\n",
    "\n",
    "## Base Models\n",
    "- Transformer Model: {metrics_transformer['Accuracy']:.4f} Accuracy, {metrics_transformer['F1 Score']:.4f} F1 Score\n",
    "- XGBoost Model: {metrics_xgb['Accuracy']:.4f} Accuracy, {metrics_xgb['F1 Score']:.4f} F1 Score\n",
    "\n",
    "## Best Ensemble Method\n",
    "- Model: {best_model}\n",
    "- F1 Score: {best_f1:.4f}\n",
    "- Improvement over best base model: {best_improvement:.2f}%\n",
    "\n",
    "## Ensemble Methods Analysis\n",
    "1. **Stacking Ensembles**: Trained a meta-model (Logistic Regression or MLP) using base model predictions as features\n",
    "2. **Weighted Ensembles**: Combined predictions with optimized weights for accuracy or F1 score\n",
    "3. **Confidence-Weighted Ensemble**: Dynamically weighted predictions based on confidence\n",
    "\n",
    "## Key Findings\n",
    "- The ensembles successfully combined the strengths of both models\n",
    "- Model agreement analysis showed complementary prediction patterns\n",
    "- Sequence context analysis revealed patterns where models succeed or fail\n",
    "- The optimal weight for the transformer model was {weight_f1:.4f} when optimizing for F1 score\n",
    "\n",
    "## Recommendations\n",
    "- Use {best_model} for best overall performance\n",
    "- Consider applying different ensemble strategies for different types of sequences\n",
    "- Further explore the sequence patterns where both models fail to improve future models\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "with open('ensemble_analysis/summary.md', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nAnalysis complete. Summary saved to ensemble_analysis/summary.md\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ba6b8-3d7c-4afb-92a9-d70f726db6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda_env]",
   "language": "python",
   "name": "conda-env-cuda_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
