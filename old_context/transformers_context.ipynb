{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f64560-70f2-4f4b-98c8-01169d88da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs('transformers_logs', exist_ok=True)\n",
    "os.makedirs('transformers_logs/tensorboard', exist_ok=True)\n",
    "os.makedirs('transformers_logs/model_checkpoints', exist_ok=True)\n",
    "os.makedirs('transformers_logs/results', exist_ok=True)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9f4a3-d8df-4654-8700-bf052f393375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences(file_path=\"Sequence_data.txt\"):\n",
    "    \"\"\"Load protein sequences from a FASTA file\"\"\"\n",
    "    print(\"Loading protein sequences...\")\n",
    "    \n",
    "    headers = []\n",
    "    sequences = []\n",
    "    current_header = None\n",
    "    current_seq = \"\"\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                # If there is an existing sequence, save it\n",
    "                if current_header:\n",
    "                    headers.append(current_header)\n",
    "                    sequences.append(current_seq)\n",
    "                \n",
    "                # Extract header ID (middle part)\n",
    "                full_header = line[1:]\n",
    "                parts = full_header.split(\"|\")\n",
    "                current_header = parts[1] if len(parts) > 1 else full_header\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line\n",
    "        \n",
    "        # Don't forget the last sequence\n",
    "        if current_header:\n",
    "            headers.append(current_header)\n",
    "            sequences.append(current_seq)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Header\": headers,\n",
    "        \"Sequence\": sequences\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(df)} protein sequences\")\n",
    "    return df\n",
    "\n",
    "def load_labels(file_path=\"labels.xlsx\"):\n",
    "    \"\"\"Load phosphorylation site labels\"\"\"\n",
    "    print(\"Loading phosphorylation site labels...\")\n",
    "    df_labels = pd.read_excel(file_path)\n",
    "    print(f\"Loaded {len(df_labels)} phosphorylation sites\")\n",
    "    return df_labels\n",
    "\n",
    "def merge_sequence_and_labels(df_seq, df_labels):\n",
    "    \"\"\"Merge sequence data with labels data\"\"\"\n",
    "    print(\"Merging sequences with labels...\")\n",
    "    \n",
    "    # Merge using pandas\n",
    "    merged_df = pd.merge(\n",
    "        df_seq,\n",
    "        df_labels,\n",
    "        left_on=\"Header\",\n",
    "        right_on=\"UniProt ID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Add target column\n",
    "    merged_df[\"target\"] = 1  # All these are positive examples\n",
    "    \n",
    "    print(f\"Merged data contains {len(merged_df)} rows\")\n",
    "    return merged_df\n",
    "\n",
    "# Load data\n",
    "df_seq = load_sequences()\n",
    "df_labels = load_labels()\n",
    "df_merged = merge_sequence_and_labels(df_seq, df_labels)\n",
    "\n",
    "# Display a few rows to check the data\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82989c5-7495-410e-89c5-e975a1ea1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import progressbar\n",
    "\n",
    "def generate_negative_samples(df_merged):\n",
    "    \"\"\"\n",
    "    Generate negative samples for each protein sequence by randomly sampling\n",
    "    from S/T/Y sites that are not known phosphorylation sites.\n",
    "    \"\"\"\n",
    "    print(\"Generating negative samples...\")\n",
    "    \n",
    "    all_rows = []\n",
    "    groups = list(df_merged.groupby('Header'))\n",
    "    \n",
    "    # Create a text-based progress bar\n",
    "    bar = progressbar.ProgressBar(max_value=len(groups))\n",
    "    \n",
    "    for i, (header, group) in enumerate(groups):\n",
    "        seq = group['Sequence'].iloc[0]\n",
    "        positive_positions = group['Position'].astype(int).tolist()\n",
    "        sty_positions = [idx+1 for idx, aa in enumerate(seq) if aa in (\"S\", \"T\", \"Y\")]\n",
    "        negative_candidates = [pos for pos in sty_positions if pos not in positive_positions]\n",
    "        \n",
    "        n_pos = len(positive_positions)\n",
    "        sample_size = min(n_pos, len(negative_candidates))\n",
    "        if sample_size > 0:\n",
    "            sampled_negatives = random.sample(negative_candidates, sample_size)\n",
    "            \n",
    "            # keep all positives\n",
    "            all_rows.append(group)\n",
    "            \n",
    "            # add negatives\n",
    "            for neg_pos in sampled_negatives:\n",
    "                new_row = group.iloc[0].copy()\n",
    "                new_row['AA']       = seq[neg_pos - 1]\n",
    "                new_row['Position'] = neg_pos\n",
    "                new_row['target']   = 0\n",
    "                all_rows.append(pd.DataFrame([new_row]))\n",
    "        \n",
    "        # advance the bar\n",
    "        bar.update(i+1)\n",
    "    \n",
    "    df_final = pd.concat(all_rows, ignore_index=True)\n",
    "    print(f\"Generated dataset with {len(df_final)} rows (positives + negatives)\")\n",
    "    return df_final\n",
    "\n",
    "# === Usage ===\n",
    "# Assuming df_merged is already defined:\n",
    "df_final = generate_negative_samples(df_merged)\n",
    "\n",
    "# Check class balance\n",
    "print(\"Class distribution:\")\n",
    "print(df_final['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935845d-63cf-4c1a-aa60-acfd3cef0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhosphorylationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, window_size=20, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        sequence = row['Sequence']\n",
    "        position = int(row['Position']) - 1  # Convert to 0-based indexing\n",
    "        target = int(row['target'])\n",
    "        \n",
    "        # Extract a window around the phosphorylation site\n",
    "        start = max(0, position - self.window_size)\n",
    "        end = min(len(sequence), position + self.window_size + 1)\n",
    "        \n",
    "        # The window centered on the target site\n",
    "        window_sequence = sequence[start:end]\n",
    "        \n",
    "        # Tokenize the sequence\n",
    "        encoding = self.tokenizer(\n",
    "            window_sequence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension added by the tokenizer\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'target': torch.tensor(target, dtype=torch.float),\n",
    "            'sequence': window_sequence,\n",
    "            'position': torch.tensor(position, dtype=torch.long),\n",
    "            'header': row['Header']\n",
    "        }\n",
    "\n",
    "def split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split the dataset into training, validation, and test sets\"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0\n",
    "    \n",
    "    # Group by Header to ensure proteins don't leak across splits\n",
    "    headers = df['Header'].unique()\n",
    "    n_headers = len(headers)\n",
    "    \n",
    "    # Shuffle the headers\n",
    "    np.random.shuffle(headers)\n",
    "    \n",
    "    # Split points\n",
    "    train_split = int(n_headers * train_ratio)\n",
    "    val_split = int(n_headers * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split headers\n",
    "    train_headers = headers[:train_split]\n",
    "    val_headers = headers[train_split:val_split]\n",
    "    test_headers = headers[val_split:]\n",
    "    \n",
    "    # Create dataframes\n",
    "    train_df = df[df['Header'].isin(train_headers)]\n",
    "    val_df = df[df['Header'].isin(val_headers)]\n",
    "    test_df = df[df['Header'].isin(test_headers)]\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples from {len(train_headers)} proteins\")\n",
    "    print(f\"Validation set: {len(val_df)} samples from {len(val_headers)} proteins\")\n",
    "    print(f\"Test set: {len(test_df)} samples from {len(test_headers)} proteins\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load the ESM-2 tokenizer - using the correct model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df, test_df = split_dataset(df_final)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PhosphorylationDataset(train_df, tokenizer, window_size=20)\n",
    "val_dataset = PhosphorylationDataset(val_df, tokenizer, window_size=20)\n",
    "test_dataset = PhosphorylationDataset(test_df, tokenizer, window_size=20)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Check a sample from the dataset\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Target shape: {sample['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ecb35-59eb-444c-ac44-d09675777f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhosphoTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/esm2_t6_8M_UR50D\", dropout_rate=0.3, window_context=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained protein language model\n",
    "        self.protein_encoder = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get hidden size from the model config\n",
    "        hidden_size = self.protein_encoder.config.hidden_size\n",
    "        \n",
    "        # Context aggregation (lightweight)\n",
    "        self.window_context = window_context\n",
    "        context_size = hidden_size * (2*window_context + 1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(context_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the transformer outputs\n",
    "        outputs = self.protein_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get sequence outputs\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Find the center position\n",
    "        center_pos = sequence_output.shape[1] // 2\n",
    "        \n",
    "        # Extract features from window around center\n",
    "        batch_size, seq_len, hidden_dim = sequence_output.shape\n",
    "        context_features = []\n",
    "        \n",
    "        for i in range(-self.window_context, self.window_context + 1):\n",
    "            pos = center_pos + i\n",
    "            # Handle boundary cases\n",
    "            if pos < 0 or pos >= seq_len:\n",
    "                # Use zero padding for out-of-bounds positions\n",
    "                context_features.append(torch.zeros(batch_size, hidden_dim, device=sequence_output.device))\n",
    "            else:\n",
    "                context_features.append(sequence_output[:, pos, :])\n",
    "        \n",
    "        # Concatenate context features\n",
    "        concat_features = torch.cat(context_features, dim=1)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(concat_features)\n",
    "        \n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f8d73-85d7-4902-8548-8228b34815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(\"Training:\")\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print progress occasionally\n",
    "        if i % 20 == 0 or i == len(data_loader) - 1:\n",
    "            print(f\"\\rBatch {i+1}/{len(data_loader)}, Loss: {loss.item():.4f}\", end=\"\")\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and targets for metrics\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predictions.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "    \n",
    "    print()  # Add a newline after the progress bar\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_predictions_binary = (np.array(all_predictions) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(all_targets, all_predictions_binary)\n",
    "    precision = precision_score(all_targets, all_predictions_binary)\n",
    "    recall = recall_score(all_targets, all_predictions_binary)\n",
    "    f1 = f1_score(all_targets, all_predictions_binary)\n",
    "    auc = roc_auc_score(all_targets, all_predictions)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(\"Evaluating:\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            # Get batch data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and targets for metrics\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if i % 20 == 0 or i == len(data_loader) - 1:\n",
    "                print(f\"\\rBatch {i+1}/{len(data_loader)}\", end=\"\")\n",
    "    \n",
    "    print()  # Add a newline after the progress\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_predictions_binary = (np.array(all_predictions) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(all_targets, all_predictions_binary)\n",
    "    precision = precision_score(all_targets, all_predictions_binary)\n",
    "    recall = recall_score(all_targets, all_predictions_binary)\n",
    "    f1 = f1_score(all_targets, all_predictions_binary)\n",
    "    auc = roc_auc_score(all_targets, all_predictions)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions_binary)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"predictions\": all_predictions,\n",
    "        \"targets\": all_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc630d83-e8e7-470d-993f-7e6f30965ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=5e-5, weight_decay=0.01):\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Total steps for scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    \n",
    "    # Set up scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Set up TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=\"transformers_logs/tensorboard\")\n",
    "    \n",
    "    # Best validation metric\n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in train_metrics.items():\n",
    "            writer.add_scalar(f\"train/{metric_name}\", metric_value, epoch)\n",
    "            print(f\"Train {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        for metric_name, metric_value in val_metrics.items():\n",
    "            if metric_name not in [\"confusion_matrix\", \"predictions\", \"targets\"]:\n",
    "                writer.add_scalar(f\"val/{metric_name}\", metric_value, epoch)\n",
    "                print(f\"Val {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        # Save metrics to log file\n",
    "        with open(f\"transformers_logs/epoch_{epoch+1}_metrics.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"train\": train_metrics,\n",
    "                \"val\": {k: v if not isinstance(v, np.ndarray) else v.tolist() \n",
    "                       for k, v in val_metrics.items() if k != \"predictions\" and k != \"targets\"}\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        # Save model if it's the best so far\n",
    "        if val_metrics[\"f1\"] > best_val_f1:\n",
    "            best_val_f1 = val_metrics[\"f1\"]\n",
    "            torch.save(model.state_dict(), f\"transformers_logs/model_checkpoints/best_model.pt\")\n",
    "            print(f\"Saved new best model with F1 score: {best_val_f1:.4f}\")\n",
    "        \n",
    "        # Always save the latest model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_metrics': train_metrics,\n",
    "            'val_metrics': {k: v for k, v in val_metrics.items() if k != \"predictions\" and k != \"targets\"}\n",
    "        }, f\"transformers_logs/model_checkpoints/latest_model.pt\")\n",
    "    \n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = PhosphoTransformer().to(device)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,  # Adjust based on your needs\n",
    "    lr= 2e-5,   # Start with a small learning rate for fine-tuning\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ac3d3-13be-412e-819a-297721958a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves():\n",
    "    \"\"\"Plot training and validation loss curves from training logs\"\"\"\n",
    "    \n",
    "    # Find all metric files from training\n",
    "    import glob\n",
    "    log_files = sorted(glob.glob(\"transformers_logs/epoch_*_metrics.json\"))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"No training log files found. Make sure you've run training first.\")\n",
    "        return\n",
    "    \n",
    "    # Extract epoch numbers from filenames\n",
    "    import re\n",
    "    epochs = [int(re.search(r\"epoch_(\\d+)_metrics\", file).group(1)) for file in log_files]\n",
    "    \n",
    "    # Load metrics from each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for file in log_files:\n",
    "        with open(file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "            train_losses.append(metrics['train']['loss'])\n",
    "            val_losses.append(metrics['val']['loss'])\n",
    "            train_accuracies.append(metrics['train']['accuracy'])\n",
    "            val_accuracies.append(metrics['val']['accuracy'])\n",
    "    \n",
    "    # Create loss curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Create accuracy curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"transformers_logs/results/training_curves.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training curves saved to transformers_logs/results/training_curves.png\")\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(\"transformers_logs/results/training_curves.png\"))\n",
    "    \n",
    "    # Also create a plot for all metrics\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        train_values = [metrics['train'][metric] for metrics in [json.load(open(f)) for f in log_files]]\n",
    "        val_values = [metrics['val'][metric] for metrics in [json.load(open(f)) for f in log_files]]\n",
    "        \n",
    "        plt.plot(epochs, train_values, 'b-', label=f'Training {metric}')\n",
    "        plt.plot(epochs, val_values, 'r-', label=f'Validation {metric}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f'Training and Validation {metric.capitalize()}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"transformers_logs/results/all_metrics_curves.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"All metrics curves saved to transformers_logs/results/all_metrics_curves.png\")\n",
    "    display(Image(\"transformers_logs/results/all_metrics_curves.png\"))\n",
    "\n",
    "# Run the function to generate the plots\n",
    "plot_training_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a6bdc-963f-4d12-accf-6e2fa845d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(model, test_loader, device):\n",
    "    # Evaluate on test set\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        if metric_name not in [\"confusion_matrix\", \"predictions\", \"targets\"]:\n",
    "            print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(\"transformers_logs/results/test_metrics.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            k: v if not isinstance(v, np.ndarray) else v.tolist() \n",
    "            for k, v in test_metrics.items() if k != \"predictions\" and k != \"targets\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Save detailed predictions for analysis\n",
    "    test_predictions = []\n",
    "    print(\"Generating detailed predictions:\")\n",
    "    with torch.no_grad():\n",
    "        batch_count = len(test_loader)\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # Print progress\n",
    "            if i % 5 == 0 or i == batch_count - 1:\n",
    "                print(f\"\\rBatch {i+1}/{batch_count}\", end=\"\")\n",
    "                \n",
    "            # Get batch data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['target'].cpu().numpy()\n",
    "            headers = batch['header']\n",
    "            positions = batch['position'].cpu().numpy()\n",
    "            sequences = batch['sequence']\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probas = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probas > 0.5).astype(int)\n",
    "            \n",
    "            # Store results\n",
    "            for i in range(len(headers)):\n",
    "                test_predictions.append({\n",
    "                    \"header\": headers[i],\n",
    "                    \"position\": int(positions[i]) + 1,  # Convert back to 1-based indexing\n",
    "                    \"sequence\": sequences[i],\n",
    "                    \"target\": int(targets[i]),\n",
    "                    \"prediction\": int(preds[i]),\n",
    "                    \"probability\": float(probas[i])\n",
    "                })\n",
    "    \n",
    "    print()  # New line after progress tracking\n",
    "    \n",
    "    # Save detailed predictions\n",
    "    pd.DataFrame(test_predictions).to_csv(\"transformers_logs/results/test_predictions.csv\", index=False)\n",
    "    \n",
    "    return test_metrics, test_predictions\n",
    "\n",
    "# Load best model\n",
    "best_model = PhosphoTransformer().to(device)\n",
    "best_model.load_state_dict(torch.load(\"transformers_logs/model_checkpoints/best_model.pt\"))\n",
    "\n",
    "# Evaluate\n",
    "test_metrics, test_predictions = evaluate_and_save_results(best_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe6028-3f2b-4edf-b017-698221324ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(targets, probas, output_path):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(targets, probas)\n",
    "    auc = roc_auc_score(targets, probas)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall_curve(targets, probas, output_path):\n",
    "    \"\"\"Plot precision-recall curve\"\"\"\n",
    "    from sklearn.metrics import average_precision_score  # Add this import\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(targets, probas)\n",
    "    avg_precision = average_precision_score(targets, probas)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(recall, precision, lw=2, label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, output_path):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(metrics, output_path):\n",
    "    \"\"\"Plot comparison of key metrics\"\"\"\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    values = [metrics[metric] for metric in metrics_to_plot]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metrics_to_plot, values, color='steelblue')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92a709-4732-44b2-b556-337e30dc64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed predictions for visualization\n",
    "test_predictions_df = pd.read_csv(\"transformers_logs/results/test_predictions.csv\")\n",
    "\n",
    "# Extract targets and predictions for visualization\n",
    "targets = test_predictions_df['target'].values\n",
    "probabilities = test_predictions_df['probability'].values\n",
    "\n",
    "# Create visualizations\n",
    "plot_roc_curve(\n",
    "    targets, \n",
    "    probabilities, \n",
    "    \"transformers_logs/results/roc_curve.png\"\n",
    ")\n",
    "\n",
    "plot_precision_recall_curve(\n",
    "    targets, \n",
    "    probabilities, \n",
    "    \"transformers_logs/results/precision_recall_curve.png\"\n",
    ")\n",
    "\n",
    "# For confusion matrix, we need to compute it from the predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(\n",
    "    test_predictions_df['target'], \n",
    "    test_predictions_df['prediction']\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm, \n",
    "    \"transformers_logs/results/confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# For metrics comparison, calculate metrics from predictions\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(test_predictions_df['target'], test_predictions_df['prediction']),\n",
    "    'precision': precision_score(test_predictions_df['target'], test_predictions_df['prediction']),\n",
    "    'recall': recall_score(test_predictions_df['target'], test_predictions_df['prediction']),\n",
    "    'f1': f1_score(test_predictions_df['target'], test_predictions_df['prediction']),\n",
    "    'auc': roc_auc_score(test_predictions_df['target'], test_predictions_df['probability'])\n",
    "}\n",
    "\n",
    "plot_metrics_comparison(\n",
    "    metrics,\n",
    "    \"transformers_logs/results/metrics_comparison.png\"\n",
    ")\n",
    "\n",
    "# Show the plots in the notebook\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"ROC Curve:\")\n",
    "display(Image(\"transformers_logs/results/roc_curve.png\"))\n",
    "\n",
    "print(\"Precision-Recall Curve:\")\n",
    "display(Image(\"transformers_logs/results/precision_recall_curve.png\"))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "display(Image(\"transformers_logs/results/confusion_matrix.png\"))\n",
    "\n",
    "print(\"Metrics Comparison:\")\n",
    "display(Image(\"transformers_logs/results/metrics_comparison.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa87cd-e83f-43cd-8b28-eb09ad492010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_misclassifications(predictions_df):\n",
    "    \"\"\"Analyze misclassified examples\"\"\"\n",
    "    # Load predictions\n",
    "    if isinstance(predictions_df, str):\n",
    "        predictions_df = pd.read_csv(predictions_df)\n",
    "    \n",
    "    # Find misclassified examples\n",
    "    misclassified = predictions_df[predictions_df['target'] != predictions_df['prediction']]\n",
    "    \n",
    "    # False positives (predicted 1, actual 0)\n",
    "    false_positives = misclassified[misclassified['prediction'] == 1]\n",
    "    \n",
    "    # False negatives (predicted 0, actual 1)\n",
    "    false_negatives = misclassified[misclassified['prediction'] == 0]\n",
    "    \n",
    "    print(f\"Total misclassifications: {len(misclassified)} ({len(misclassified)/len(predictions_df)*100:.2f}% of test set)\")\n",
    "    print(f\"False positives: {len(false_positives)} ({len(false_positives)/len(misclassified)*100:.2f}% of errors)\")\n",
    "    print(f\"False negatives: {len(false_negatives)} ({len(false_negatives)/len(misclassified)*100:.2f}% of errors)\")\n",
    "    \n",
    "    # Look at the most confident misclassifications\n",
    "    most_confident_fp = false_positives.sort_values('probability', ascending=False).head(10)\n",
    "    most_confident_fn = false_negatives.sort_values('probability').head(10)\n",
    "    \n",
    "    print(\"\\nMost confident false positives:\")\n",
    "    print(most_confident_fp[['header', 'position', 'sequence', 'probability']])\n",
    "    \n",
    "    print(\"\\nMost confident false negatives:\")\n",
    "    print(most_confident_fn[['header', 'position', 'sequence', 'probability']])\n",
    "    \n",
    "    # Analyze sequence patterns in misclassifications\n",
    "    # Count central amino acid types\n",
    "    center_aa_fp = [seq[len(seq)//2] for seq in false_positives['sequence']]\n",
    "    center_aa_fn = [seq[len(seq)//2] for seq in false_negatives['sequence']]\n",
    "    \n",
    "    fp_aa_counts = pd.Series(center_aa_fp).value_counts()\n",
    "    fn_aa_counts = pd.Series(center_aa_fn).value_counts()\n",
    "    \n",
    "    print(\"\\nCentral amino acid distribution in false positives:\")\n",
    "    print(fp_aa_counts)\n",
    "    \n",
    "    print(\"\\nCentral amino acid distribution in false negatives:\")\n",
    "    print(fn_aa_counts)\n",
    "    \n",
    "    # Save analysis to file\n",
    "    with open(\"transformers_logs/results/misclassification_analysis.txt\", \"w\") as f:\n",
    "        f.write(f\"Total misclassifications: {len(misclassified)} ({len(misclassified)/len(predictions_df)*100:.2f}% of test set)\\n\")\n",
    "        f.write(f\"False positives: {len(false_positives)} ({len(false_positives)/len(misclassified)*100:.2f}% of errors)\\n\")\n",
    "        f.write(f\"False negatives: {len(false_negatives)} ({len(false_negatives)/len(misclassified)*100:.2f}% of errors)\\n\\n\")\n",
    "        \n",
    "        f.write(\"Most confident false positives:\\n\")\n",
    "        f.write(most_confident_fp[['header', 'position', 'sequence', 'probability']].to_string() + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Most confident false negatives:\\n\")\n",
    "        f.write(most_confident_fn[['header', 'position', 'sequence', 'probability']].to_string() + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Central amino acid distribution in false positives:\\n\")\n",
    "        f.write(fp_aa_counts.to_string() + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Central amino acid distribution in false negatives:\\n\")\n",
    "        f.write(fn_aa_counts.to_string() + \"\\n\")\n",
    "    \n",
    "    return misclassified\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassified = analyze_misclassifications(\"transformers_logs/results/test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660302ab-1536-4f9d-8bcf-e8b8ffb00644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_xgboost():\n",
    "    \"\"\"\n",
    "    Compare transformer model performance with XGBoost baseline\n",
    "    Note: This assumes you have previously run the XGBoost model and have results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load XGBoost results\n",
    "        with open(\"results/metrics_20250414_195426.json\", \"r\") as f:\n",
    "            xgboost_metrics = json.load(f)\n",
    "        \n",
    "        # Load transformer results\n",
    "        with open(\"transformers_logs/results/test_metrics.json\", \"r\") as f:\n",
    "            transformer_metrics = json.load(f)\n",
    "        \n",
    "        # Extract metrics for comparison\n",
    "        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "        \n",
    "        xgb_values = [xgboost_metrics.get(m.capitalize(), xgboost_metrics.get(m.upper(), xgboost_metrics.get(m, 0))) \n",
    "                     for m in metrics_to_compare]\n",
    "        transformer_values = [transformer_metrics[m] for m in metrics_to_compare]\n",
    "        \n",
    "        # Create comparison bar chart\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        x = np.arange(len(metrics_to_compare))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_values, width, label='XGBoost')\n",
    "        plt.bar(x + width/2, transformer_values, width, label='Transformer')\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Scores')\n",
    "        plt.title('Performance Comparison: XGBoost vs Transformer')\n",
    "        plt.xticks(x, metrics_to_compare)\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(xgb_values):\n",
    "            plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "            \n",
    "        for i, v in enumerate(transformer_values):\n",
    "            plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"transformers_logs/results/model_comparison.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"XGBoost vs Transformer comparison:\")\n",
    "        display(Image(\"transformers_logs/results/model_comparison.png\"))\n",
    "        \n",
    "        # Save comparison to text file\n",
    "        with open(\"transformers_logs/results/model_comparison.txt\", \"w\") as f:\n",
    "            f.write(\"Performance Comparison: XGBoost vs Transformer\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            for i, metric in enumerate(metrics_to_compare):\n",
    "                f.write(f\"{metric.capitalize()}:\\n\")\n",
    "                f.write(f\"  - XGBoost:     {xgb_values[i]:.4f}\\n\")\n",
    "                f.write(f\"  - Transformer: {transformer_values[i]:.4f}\\n\")\n",
    "                f.write(f\"  - Difference:  {transformer_values[i] - xgb_values[i]:.4f}\\n\\n\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare with XGBoost: {e}\")\n",
    "        return False\n",
    "\n",
    "# Compare with XGBoost if available\n",
    "try:\n",
    "    compare_with_xgboost()\n",
    "except:\n",
    "    print(\"XGBoost comparison failed or not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a1c99-e3ea-479a-aa82-22c33bb51383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phosphorylation(model, tokenizer, sequence, positions=None, window_size=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Predict phosphorylation sites in a protein sequence\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        sequence: Protein sequence as string\n",
    "        positions: List of positions to check (1-based indexing). If None, check all S/T/Y positions.\n",
    "        window_size: Window size to use for prediction\n",
    "        device: Device to run predictions on\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # If positions not specified, find all S/T/Y positions\n",
    "    if positions is None:\n",
    "        positions = [i+1 for i, aa in enumerate(sequence) if aa in [\"S\", \"T\", \"Y\"]]\n",
    "    \n",
    "    results = []\n",
    "    total = len(positions)\n",
    "    \n",
    "    print(f\"Predicting phosphorylation for {total} positions:\")\n",
    "    with torch.no_grad():\n",
    "        for i, pos in enumerate(positions):\n",
    "            # Print progress\n",
    "            if i % 10 == 0 or i == total - 1:\n",
    "                print(f\"\\rPosition {i+1}/{total}\", end=\"\")\n",
    "                \n",
    "            # Extract window\n",
    "            pos_idx = pos - 1  # Convert to 0-based\n",
    "            start = max(0, pos_idx - window_size)\n",
    "            end = min(len(sequence), pos_idx + window_size + 1)\n",
    "            window = sequence[start:end]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = tokenizer(\n",
    "                window,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Predict\n",
    "            output = model(input_ids, attention_mask)\n",
    "            probability = torch.sigmoid(output).item()\n",
    "            prediction = 1 if probability > 0.5 else 0\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"position\": pos,\n",
    "                \"amino_acid\": sequence[pos_idx],\n",
    "                \"window\": window,\n",
    "                \"probability\": probability,\n",
    "                \"prediction\": prediction\n",
    "            })\n",
    "    \n",
    "    print()  # New line after progress tracking\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Predict phosphorylation on a test sequence\n",
    "# Replace with actual test sequence from your data\n",
    "test_seq = df_final['Sequence'].iloc[0]  \n",
    "test_positions = [int(pos) for pos in df_final[df_final['Sequence'] == test_seq]['Position']]\n",
    "\n",
    "predictions = predict_phosphorylation(\n",
    "    model=best_model,\n",
    "    tokenizer=tokenizer,\n",
    "    sequence=test_seq,\n",
    "    positions=test_positions,\n",
    "    window_size=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Prediction results for test sequence:\")\n",
    "print(predictions.head(10))\n",
    "\n",
    "# Save predictions\n",
    "predictions.to_csv(\"transformers_logs/results/example_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2f4ad-1739-4820-9d9e-3fbc8f5b0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test metrics for summary\n",
    "with open(\"transformers_logs/results/test_metrics.json\", \"r\") as f:\n",
    "    test_metrics = json.load(f)\n",
    "\n",
    "# Generate summary\n",
    "summary = f\"\"\"\n",
    "# Phosphorylation Site Prediction with Transformers: Summary\n",
    "\n",
    "## Model Architecture\n",
    "- Pre-trained protein language model: ESM-2\n",
    "- Fine-tuned on phosphorylation site data\n",
    "- Window size: 10 amino acids on each side of the site\n",
    "\n",
    "## Dataset Statistics\n",
    "- Training samples: {len(train_dataset)}\n",
    "- Validation samples: {len(val_dataset)}\n",
    "- Test samples: {len(test_dataset)}\n",
    "\n",
    "## Performance Metrics\n",
    "- Accuracy: {test_metrics['accuracy']:.4f}\n",
    "- Precision: {test_metrics['precision']:.4f}\n",
    "- Recall: {test_metrics['recall']:.4f}\n",
    "- F1 Score: {test_metrics['f1']:.4f}\n",
    "- ROC AUC: {test_metrics['auc']:.4f}\n",
    "\n",
    "## Confusion Matrix\n",
    "{test_metrics['confusion_matrix']}\n",
    "\n",
    "## Key Findings\n",
    "- The transformer-based model achieves good performance on phosphorylation site prediction\n",
    "- The model leverages pre-trained protein language model knowledge\n",
    "- Misclassification analysis shows patterns that could be addressed in future work\n",
    "\n",
    "## Advantages Over Feature Engineering Approaches\n",
    "- No manual feature engineering required\n",
    "- Captures complex patterns and long-range dependencies in protein sequences\n",
    "- Can be fine-tuned with relatively small datasets\n",
    "\n",
    "## Future Improvements\n",
    "- Try different protein language models (ESM-2 larger variants, ProtTrans, etc.)\n",
    "- Combine with structural information using Graph Neural Networks\n",
    "- Experiment with different window sizes and model architectures\n",
    "- Incorporate kinase-specific information for better predictions\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "with open(\"transformers_logs/results/final_summary.md\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "# Print summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa336e6-5f16-4071-b35c-f56ce264a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=transformers_logs/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089253af-dccc-4188-9188-3c744d098bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5288d-a55f-478f-ba7b-58bb94479258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87175e4e-e5f6-44e8-9cbb-62581fba16f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda_env]",
   "language": "python",
   "name": "conda-env-cuda_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
